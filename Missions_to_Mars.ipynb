{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import pymongo\n",
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Database in MongoDB**\n",
    "![title](Images/mongo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Connect to Mongo DB Mars DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars\n",
    "collection = db.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get executable_path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1 - Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NASA Mars News**\n",
    "\n",
    "Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_nasa_news():\n",
    "    mars_news_data = []\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "    browser.visit(url)\n",
    "    #need timer to ensure page has load before scraping?\n",
    "    time.sleep(5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # data structures\n",
    "    posting = {}\n",
    "    titles = []\n",
    "    content = []   \n",
    "    \n",
    "    title_results = soup.find_all(\"div\", {\"class\": \"content_title\"})\n",
    "    teaser_results = soup.find_all(\"div\", {\"class\": \"article_teaser_body\"})\n",
    "\n",
    "    for result in title_results:\n",
    "        title = result.find('a').text\n",
    "        titles.append(title)\n",
    "\n",
    "    for teaser in teaser_results:\n",
    "        b = teaser.get_text()\n",
    "        content.append(b)\n",
    "    # not all 'content_title' tags have 'article_teaser_body' content\n",
    "    # loop through all content, ignoring 'content_title without data'\n",
    "    a = (min( len(titles), len(content) ))\n",
    "    #print (a)\n",
    "    \n",
    "    for i in range(1):\n",
    "        posting = {'title': titles[i], 'text': content[i]}\n",
    "        mars_news_data.append(posting)\n",
    "        #coll.insert_one(posting)\n",
    "        \n",
    "    return mars_news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JPL Mars Space Images - Featured Image**\n",
    "\n",
    "Latest Mars image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_mars_image():\n",
    "    url_mars = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    browser.visit(url_mars)\n",
    "    #need timer to ensure page has load before scraping?\n",
    "    time.sleep(5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    image = soup.find(\"a\", {\"class\": \"button fancybox\"})\n",
    "    med_size = image.attrs['data-fancybox-href']\n",
    "    large_size = med_size.replace('mediumsize', 'largesize')\n",
    "    large_size = large_size.replace('_ip', '_hires')\n",
    "    images_link = 'https://www.jpl.nasa.gov' + large_size\n",
    "    \n",
    "    return images_link\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Latest Mars Weather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_mars_weather():\n",
    "    url_mars_weather = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    browser.visit(url_mars_weather)\n",
    "    #need timer to ensure page has load before scraping?\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(browser.html, 'html.parser')   \n",
    "    weather_container = soup.find(\"p\", {\"class\": \"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"})\n",
    "    forecast = weather_container.text\n",
    "    wc = weather_container.find('a').text\n",
    "    \n",
    "    wetter = forecast.replace(weather_container.find('a').text, '')\n",
    "    \n",
    "    return wetter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mars Table Facts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    " def mars_facts():\n",
    "    mars_facts_url = 'https://space-facts.com/mars/'\n",
    "    import pandas as pd\n",
    "    \n",
    "    #browser = Browser()\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    browser.visit(mars_facts_url)\n",
    "    time.sleep(5)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    mars_facts_table = soup.find(\"table\", {\"class\": \"tablepress tablepress-id-p-mars\"})\n",
    "    df_mars_facts = pd.read_html(str(mars_facts_table))\n",
    "\n",
    "    html_table = df_mars_facts\n",
    "    \n",
    "    return df_mars_facts\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mars Hemispheres**\n",
    "\n",
    "Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "\n",
    "You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "\n",
    "Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "\n",
    "\n",
    "Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_image_url():\n",
    "    img_url = []\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url)\n",
    "    #need a pause to ensure page has load before scraping?\n",
    "    time.sleep(7)\n",
    "    soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "    astrogeology_items = soup.find_all(\"div\", {\"class\": \"description\"})\n",
    "    img_url = []\n",
    "    for astro in astrogeology_items:\n",
    "        rel_link = astro.find('a')\n",
    "        img_url.append(rel_link.attrs['href'])\n",
    "    \n",
    "    return(img_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_hemispheres(url, hemispheres_list):\n",
    "    hemi_dict = {}\n",
    "    base_asrogeology_url = 'https://astrogeology.usgs.gov'\n",
    "    astro_url = base_asrogeology_url + url\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    browser.visit(astro_url)\n",
    "    hemi_image_urls = {}\n",
    "\n",
    "    time.sleep(8)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    link_section = soup.find_all(\"section\", {\"class\": \"block metadata\"})\n",
    "    for section in link_section:\n",
    "        print(section.find('h2').text)\n",
    "        alink = section.find('a')\n",
    "        print(alink.attrs['href'])\n",
    "        hemi_dict['title'] = section.find('h2').text\n",
    "        hemi_dict['img_url'] = alink.attrs['href']\n",
    "        hemispheres_list.append(hemi_dict)\n",
    "\n",
    "    \n",
    "    return hemispheres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_data():\n",
    "    mars_scraper = {}\n",
    "    \n",
    "    mars_news_list = []\n",
    "    hemi_image_urls = []\n",
    "    \n",
    "    featured_image_url = latest_mars_image()\n",
    "    mars_weather_string = latest_mars_weather()\n",
    "    mars_news_list = latest_nasa_news()\n",
    "    mars_facts_df = mars_facts()\n",
    "    \n",
    "    astrogeology_relative_links = mars_image_url()\n",
    "    for astro in astrogeology_relative_links:\n",
    "        hemi_image_urls = mars_hemispheres(astro, hemi_image_urls)\n",
    "    \n",
    "\n",
    "    \n",
    "    mars_scraper['news_title'] = mars_news_list[0]['title']\n",
    "    print \n",
    "    mars_scraper['news_p'] = mars_news_list[0]['text']\n",
    "    mars_scraper['featured_image_url'] = featured_image_url\n",
    "    mars_scraper['mars_weather'] = mars_weather_string\n",
    "    mars_scraper['mars_df'] = str(mars_facts_df)\n",
    "    mars_scraper['hemisphere_image_urls'] = hemi_image_urls\n",
    "    \n",
    "    \n",
    "    \n",
    "    return mars_scraper\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif\n",
      "Valles Marineris Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif\n"
     ]
    }
   ],
   "source": [
    "mission_mars_all_data = scraper_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Notebook to Python File .py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script scraper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 - MongoDB and Flask Application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
